{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d320cf6",
   "metadata": {},
   "source": [
    "## Permutation Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a273cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "credit_score=pd.read_csv(\"data/credit_score.csv\")\n",
    "\n",
    "# Select features\n",
    "features=['INCOME', 'DEBT', 'R_EXPENDITURE', 'R_ENTERTAINMENT', 'CAT_GAMBLING']\n",
    "x=credit_score[features].copy()\n",
    "\n",
    "x['GAMBLING_LOW']=x['CAT_GAMBLING'].apply(lambda x: 1 if x=='Low' else 0)\n",
    "x['GAMBLING_HIGH']=x['CAT_GAMBLING'].apply(lambda x: 1 if x=='High' else 0)\n",
    "x.drop(columns=['CAT_GAMBLING'], inplace=True)\n",
    "\n",
    "# Target variable\n",
    "y=credit_score['CREDIT_SCORE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c365604",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Step 1: Importing XGBoost Regressor\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "```\n",
    "\n",
    "* `xgb.XGBRegressor` â†’ This is the **regression model** from the XGBoost library.\n",
    "* XGBoost (**Extreme Gradient Boosting**) is a very popular machine learning algorithm that builds an **ensemble of decision trees** in a boosting framework.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 2: Creating the Model\n",
    "\n",
    "```python\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    max_depth=3,\n",
    "    n_estimators=100\n",
    ")\n",
    "```\n",
    "\n",
    "1. **`objective='reg:squarederror'`**\n",
    "\n",
    "   * Defines the type of learning task.\n",
    "   * `\"reg:squarederror\"` means the model will optimize for **Mean Squared Error (MSE)**, which is the standard for regression problems.\n",
    "   * In other words, it tries to minimize the squared difference between predicted values and actual values.\n",
    "\n",
    "2. **`max_depth=3`**\n",
    "\n",
    "   * Controls the **maximum depth of each decision tree** in the boosting process.\n",
    "   * A lower depth (like 3) â†’ simpler trees, less risk of overfitting, but possibly underfitting.\n",
    "   * A higher depth â†’ more complex trees, better training fit, but risk of overfitting.\n",
    "\n",
    "3. **`n_estimators=100`**\n",
    "\n",
    "   * Number of **boosted trees (weak learners)** to build.\n",
    "   * Each new tree corrects the errors made by the previous ones.\n",
    "   * More trees generally improve performance but increase training time and risk of overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 3: Training the Model\n",
    "\n",
    "```python\n",
    "model.fit(x, y)\n",
    "```\n",
    "\n",
    "* `.fit()` is the method to **train the model**.\n",
    "\n",
    "* Here:\n",
    "\n",
    "  * `x` â†’ Feature matrix (independent variables, input data).\n",
    "    Example: income, debt ratio, etc.\n",
    "  * `y` â†’ Target variable (dependent variable).\n",
    "    Example: credit score, house price, etc.\n",
    "\n",
    "* During training:\n",
    "\n",
    "  1. The model builds an initial prediction (like an average of all `y` values).\n",
    "  2. It calculates the **residuals** (errors between prediction and actual `y`).\n",
    "  3. It fits a decision tree to predict those residuals.\n",
    "  4. Repeats this for `n_estimators` (100 trees here).\n",
    "  5. Each tree is shallow (depth 3), making it a **weak learner**.\n",
    "  6. Boosting combines them into a strong predictor.\n",
    "\n",
    "After this step, `model` has **learned patterns from the data** and is ready to make predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6321776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model=xgb.XGBRegressor(objective='reg:squarederror', max_depth=3, n_estimators=100)\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "y_pred=model.predict(x)\n",
    "\n",
    "# Model evaluation\n",
    "fig, ax=plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "\n",
    "plt.scatter(y, y_pred)\n",
    "# Line of perfect predictions\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], color='tab:red')\n",
    "\n",
    "plt.ylabel('Predicted values', size=20)\n",
    "plt.xlabel('Actual values', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28649ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R^2 value to evaluate performance\n",
    "baseline_score=model.score(x, y)\n",
    "# The score when no feature has been permuted\n",
    "print(baseline_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7fd16",
   "metadata": {},
   "source": [
    "### Permuting a feature\n",
    "\n",
    "- `Permuting a feature` means shuffling the values of that feature. We must not sample from another feature or distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f19214",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_perm=x.copy()\n",
    "x_perm['INCOME']=np.random.permutation(x_perm['INCOME'])\n",
    "\n",
    "# Get predictions\n",
    "y_pred=model.predict(x_perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ceee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "fig, ax=plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "\n",
    "plt.scatter(y, y_pred)\n",
    "# Line of perfect predictions\n",
    "ax.plot([y.min(), y.max()], [y.min(), y.max()], color='tab:red')\n",
    "\n",
    "plt.ylabel('Predicted values', size=20)\n",
    "plt.xlabel('Actual values', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8f9d9d",
   "metadata": {},
   "source": [
    "- Since the scatter plot has been deviated a lot when we permuted `INCOME`, so we can say that `INCOME` feature contributes the most for the model prediction.\n",
    "\n",
    "- The model is using the relationship between income and credit_score to make predictions. When we permute income, we break this relationship and so the model makes worse predictions.\n",
    "\n",
    "- It is difficult to make an objective judgements on how much worse by using a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f587ead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_score=model.score(x_perm, y)\n",
    "print(permuted_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f83028",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_score=baseline_score-permuted_score\n",
    "print(importance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e1414",
   "metadata": {},
   "source": [
    "### Permutation feature importance from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef051bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perm_importance(model, x, y, features, n=10):\n",
    "    # Calculate baseline score (without permuting any feature)\n",
    "    baseline_score=model.score(x, y)\n",
    "\n",
    "    importance_scores={}\n",
    "\n",
    "    for feature in features:\n",
    "        x_perm=x.copy()\n",
    "        sum_score=0\n",
    "\n",
    "        # Repeat n times to get average importance score\n",
    "        for i in range(n):\n",
    "            # Calculate score when given feature is permuted\n",
    "            x_perm[feature]=np.random.permutation(x_perm[feature])\n",
    "            permuted_score=model.score(x_perm, y)\n",
    "\n",
    "            sum_score+=permuted_score\n",
    "\n",
    "        # Calculate decrease in score\n",
    "        importance_score=baseline_score-(sum_score/n)\n",
    "        importance_scores[feature]=importance_score\n",
    "\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a994e1",
   "metadata": {},
   "source": [
    "```python\n",
    "sorted_importance_scores = sorted(importance_scores.items(), key=lambda x: x[1])\n",
    "```\n",
    "\n",
    "1. **`importance_scores`**\n",
    "\n",
    "   * Likely a **dictionary** like:\n",
    "\n",
    "     ```python\n",
    "     importance_scores = {\"Income\": 0.45, \"Debt\": 0.25, \"Age\": 0.30}\n",
    "     ```\n",
    "   * Keys â†’ feature names\n",
    "   * Values â†’ importance score (how much that feature contributes to the model).\n",
    "\n",
    "2. **`.items()`**\n",
    "\n",
    "   * Converts the dictionary into a list of **(key, value) tuples**:\n",
    "\n",
    "     ```python\n",
    "     dict.items() â†’ [(\"Income\", 0.45), (\"Debt\", 0.25), (\"Age\", 0.30)]\n",
    "     ```\n",
    "\n",
    "3. **`sorted(..., key=lambda x: x[1])`**\n",
    "\n",
    "   * `sorted()` sorts the list of tuples.\n",
    "   * `key=lambda x: x[1]` â†’ tells Python to sort by the **second element of each tuple** (the importance score, not the feature name).\n",
    "   * So the result is sorted **ascending by score**:\n",
    "\n",
    "     ```python\n",
    "     sorted_importance_scores =\n",
    "       [(\"Debt\", 0.25), (\"Age\", 0.30), (\"Income\", 0.45)]\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "features, scores = zip(*sorted_importance_scores)\n",
    "```\n",
    "\n",
    "1. **`zip(*sorted_importance_scores)`**\n",
    "\n",
    "   * `*` operator unpacks the list of tuples.\n",
    "   * Itâ€™s like doing:\n",
    "\n",
    "     ```python\n",
    "     zip((\"Debt\", 0.25), (\"Age\", 0.30), (\"Income\", 0.45))\n",
    "     ```\n",
    "   * `zip` groups elements **by position**:\n",
    "\n",
    "     * First elements together â†’ `(\"Debt\", \"Age\", \"Income\")`\n",
    "     * Second elements together â†’ `(0.25, 0.30, 0.45)`\n",
    "\n",
    "2. **`features, scores = ...`**\n",
    "\n",
    "   * Unpacks the zipped result into two variables:\n",
    "\n",
    "     ```python\n",
    "     features = (\"Debt\", \"Age\", \"Income\")\n",
    "     scores   = (0.25, 0.30, 0.45)\n",
    "     ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation feature importance\n",
    "importance_scores=get_perm_importance(\n",
    "    model=model,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    features=x.columns,\n",
    "    n=10\n",
    ")\n",
    "\n",
    "# Display the importance scores using the bar plot\n",
    "sorted_importance_scores=sorted(importance_scores.items(), key=lambda x: x[1])\n",
    "features, scores=zip(*sorted_importance_scores)\n",
    "\n",
    "plt.subplots(figsize=(8, 4))\n",
    "plt.barh(features, scores)\n",
    "plt.xlabel('Permutation Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced38da",
   "metadata": {},
   "source": [
    "- We can conclude that `INCOME` and `DEBT` are most important feature whivh contribute for model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
