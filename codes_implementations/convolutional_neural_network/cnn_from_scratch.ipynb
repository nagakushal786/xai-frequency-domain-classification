{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c035fb",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc5175",
   "metadata": {},
   "source": [
    "### Coding a Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a99df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[1, 2, 3]\n",
    "weights=[0.2, 0.8, -0.5]\n",
    "bias=2\n",
    "\n",
    "output=inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a6e05",
   "metadata": {},
   "source": [
    "### Coding a Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[1, 2, 3, 2.5]\n",
    "weights=[\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "biases=[2, 3, 0.5]\n",
    "\n",
    "weights1=weights[0] # w11, w12, w13, w14\n",
    "weights2=weights[1] # w21, w22, w23, w24\n",
    "weights3=weights[2] # w31, w32, w33, w34\n",
    "\n",
    "bias1=biases[0] # b1\n",
    "bias2=biases[1] # b2\n",
    "bias3=biases[2] # b3\n",
    "\n",
    "outputs=[\n",
    "    # Neuron 1\n",
    "    inputs[0]*weights1[0]+\n",
    "    inputs[1]*weights1[1]+\n",
    "    inputs[2]*weights1[2]+\n",
    "    inputs[3]*weights1[3]+bias1,\n",
    "    # Neuron 2\n",
    "    inputs[0]*weights2[0]+\n",
    "    inputs[1]*weights2[1]+\n",
    "    inputs[2]*weights2[2]+\n",
    "    inputs[3]*weights2[3]+bias2,\n",
    "    # Neuron 3\n",
    "    inputs[0]*weights3[0]+\n",
    "    inputs[1]*weights3[1]+\n",
    "    inputs[2]*weights3[2]+\n",
    "    inputs[3]*weights3[3]+bias3\n",
    "]\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c79290",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs=[]\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output=0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output+=n_input*weight\n",
    "    neuron_output+=neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23622157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron through numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "inputs=np.array([1.0, 2.0, 3.0, 2.5])\n",
    "weights=np.array([0.2, 0.8, -0.5, 1.0])\n",
    "bias=2.0\n",
    "\n",
    "outputs=np.dot(inputs, weights)+bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer through numpy\n",
    "\n",
    "inputs=np.array([1, 2, 3, 2.5])\n",
    "weights=np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "])\n",
    "biases=[2, 3, 0.5]\n",
    "\n",
    "outputs=np.dot(weights, inputs)+biases\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63639a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch through numpy\n",
    "\n",
    "inputs=np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "weights=[\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "biases=[2, 3, 0.5]\n",
    "\n",
    "# The biases array which is 1x3 replicates the row twice to become 3x3 - Broadcasting operation\n",
    "# This helps for matrix addition\n",
    "outputs=np.dot(inputs, np.array(weights).T)+biases\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7840765",
   "metadata": {},
   "source": [
    "### Coding Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60871f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "inputs=np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "# Hidden Layer 1\n",
    "weights_1=[\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "bias_1=[2, 3, 0.5]\n",
    "\n",
    "# Hidden Layer 2\n",
    "weights_2=[\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]\n",
    "]\n",
    "bias_2=[-1, 2, -0.5]\n",
    "\n",
    "output_1=np.dot(inputs, np.array(weights_1).T)+bias_1\n",
    "print(f\"Output of 1st hidden layer:\\n{output_1}\\n\")\n",
    "\n",
    "output_2=np.dot(output_1, np.array(weights_2).T)+bias_2\n",
    "print(f\"Output of 2nd hidden layer:\\n{output_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ef70c",
   "metadata": {},
   "source": [
    "### Best Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf111573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Layer\n",
    "inputs=[\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "# Hidden Layer 1\n",
    "weights_1=[\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]\n",
    "bias_1=[2, 3, 0.5]\n",
    "\n",
    "# Hidden Layer 2\n",
    "weights_2=[\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13]\n",
    "]\n",
    "bias_2=[-1, 2, -0.5]\n",
    "\n",
    "input_array=np.array(inputs)\n",
    "weight1_array=np.array(weights_1)\n",
    "bias1_array=np.array(bias_1)\n",
    "weight2_array=np.array(weights_2)\n",
    "bias2_array=np.array(bias_2)\n",
    "\n",
    "layer_1_output=np.dot(input_array, weight1_array.T)+bias1_array\n",
    "layer_2_output=np.dot(layer_1_output, weight2_array.T)+bias2_array\n",
    "print(layer_2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e003952",
   "metadata": {},
   "source": [
    "### Generating Non-Linear Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66488b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnfs.datasets import spiral_data\n",
    "import nnfs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# 100 samples for each class\n",
    "x, y=spiral_data(samples=100, classes=3)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea797f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], x[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9552240",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap=\"brg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbbe2fc",
   "metadata": {},
   "source": [
    "### Coding a Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af797868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights=0.01*np.random.randn(n_neurons, n_inputs)\n",
    "        self.biases=np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output=np.dot(inputs, self.weights.T)+self.biases\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create a dense layer with 2 input features and 3 neurons\n",
    "dense_1=DenseLayer(2, 3)\n",
    "# Perform forward pass on our training data through this layer\n",
    "dense_1.forward(x)\n",
    "# Shape of the output\n",
    "print(dense_1.output.shape)\n",
    "# Getting the outputs of first few samples\n",
    "print(dense_1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e0dde",
   "metadata": {},
   "source": [
    "### Array Summation in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f25fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35654fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum=np.sum(a)\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9327d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_sum=np.sum(a, axis=0)\n",
    "print(column_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748cadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum=np.sum(a, axis=1)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd6bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_sum.shape, column_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58db758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To retain the dimension from (3,) to (3,1)\n",
    "column_sum=np.sum(a, axis=0, keepdims=True)\n",
    "row_sum=np.sum(a, axis=1, keepdims=True)\n",
    "\n",
    "print(column_sum)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_sum.shape, column_sum.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f983e6",
   "metadata": {},
   "source": [
    "### Broadcasting Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16512215",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_1=np.max(a, axis=1, keepdims=True)\n",
    "print(b_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3824ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_2=np.max(a, axis=0, keepdims=True)\n",
    "print(b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71651c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ans_1=a-b_1\n",
    "print(final_ans_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ans_2=a-b_2\n",
    "print(final_ans_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf568c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array([1, 2, 3])\n",
    "\n",
    "ans=a+arr\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e4501",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf333001",
   "metadata": {},
   "source": [
    "#### ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a31265",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output=np.maximum(0, inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output=np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d99b82",
   "metadata": {},
   "source": [
    "#### Softmax Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "]\n",
    "\n",
    "# Get unnormalized probabilities\n",
    "exp_values=np.exp(inputs-np.max(inputs, axis=1, keepdims=True))\n",
    "# Normalize for each sample\n",
    "probabilities=exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "print(probabilities)\n",
    "print(np.sum(probabilities, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07369b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationSoftmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        exp_values=np.exp(inputs-np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities=exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output=probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d959f",
   "metadata": {},
   "source": [
    "### Coding a Forward Pass - Without Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inputs: {x.shape}\")\n",
    "\n",
    "dense_1=DenseLayer(n_inputs=2, n_neurons=3)\n",
    "activation_1=ActivationReLU()\n",
    "\n",
    "dense_2=DenseLayer(n_inputs=3, n_neurons=3)\n",
    "activation_2=ActivationSoftmax()\n",
    "\n",
    "dense_1.forward(x)\n",
    "activation_1.forward(dense_1.output)\n",
    "\n",
    "dense_2.forward(activation_1.output)\n",
    "activation_2.forward(dense_2.output)\n",
    "\n",
    "print(f\"Outputs: {activation_2.output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d066168",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a787bd5c",
   "metadata": {},
   "source": [
    "`Case 1`: Class targets are just numbers\n",
    "\n",
    "`Red`: 0, `Green`: 1, `Blue`: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c0fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs=np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "class_targets=[0, 1, 1]\n",
    "\n",
    "print(softmax_outputs[[0, 1, 2], class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_log=-np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "average_loss=np.mean(neg_log)\n",
    "print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eeac22",
   "metadata": {},
   "source": [
    "`Case 2`: Class targets are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee73de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_check=np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "y_pred_check=np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "y_probs=y_true_check*y_pred_check\n",
    "y_req_probs=np.sum(y_probs, axis=1)\n",
    "req_outputs=-np.log(y_req_probs)\n",
    "average_loss=np.mean(req_outputs)\n",
    "\n",
    "print(f\"Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f5f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    # Calculate the regularization loss with given model outputs and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses=self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss=np.mean(sample_losses)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples=len(y_pred)\n",
    "\n",
    "        # Clip data to prevent 0 division\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped=np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # For categorical targets\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences=y_pred_clipped[np.arange(samples), y_true]\n",
    "        # For one-hot encoded targets\n",
    "        elif len(y_true.shape)==2:\n",
    "            correct_confidences=np.sum(\n",
    "                y_pred_clipped*y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        neg_log_likelihood=-np.log(correct_confidences)\n",
    "        return neg_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0564e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs=np.array([\n",
    "    [0.7, 0.1, 0.2],\n",
    "    [0.1, 0.5, 0.4],\n",
    "    [0.02, 0.9, 0.08]\n",
    "])\n",
    "\n",
    "class_targets=np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "loss_function=CategoricalCrossEntropyLoss()\n",
    "\n",
    "loss=loss_function.calculate(softmax_outputs, class_targets)\n",
    "print(f\"Categorical Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8e7b1d",
   "metadata": {},
   "source": [
    "### Coding a Forward Pass - With Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_1=DenseLayer(2, 3)\n",
    "activation_1=ActivationReLU()\n",
    "\n",
    "dense_2=DenseLayer(3, 3)\n",
    "activation_2=ActivationSoftmax()\n",
    "\n",
    "loss_func=CategoricalCrossEntropyLoss()\n",
    "\n",
    "dense_1.forward(x)\n",
    "activation_1.forward(dense_1.output)\n",
    "\n",
    "dense_2.forward(activation_1.output)\n",
    "activation_2.forward(dense_2.output)\n",
    "\n",
    "loss=loss_func.calculate(activation_2.output, y)\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the accuracy\n",
    "predictions=np.argmax(activation_2.output, axis=1)\n",
    "\n",
    "if len(y.shape)==2:\n",
    "    y=np.argmax(y, axis=1)\n",
    "\n",
    "accuracy=np.mean(predictions==y)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a23ea",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15a0e0",
   "metadata": {},
   "source": [
    "#### Randomly choosing weights and biases - Doesn't work for any kind of data\n",
    "\n",
    "- Pick weights and biases randomly.\n",
    "\n",
    "- Calculate the loss.\n",
    "\n",
    "- Iterate.\n",
    "\n",
    "- Choose weights and biases which gives the lowest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa500b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_1=DenseLayer(2, 3)\n",
    "activation_1=ActivationReLU()\n",
    "\n",
    "dense_2=DenseLayer(3, 3)\n",
    "activation_2=ActivationSoftmax()\n",
    "\n",
    "loss_func=CategoricalCrossEntropyLoss()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss=9999999\n",
    "best_dense1_weights=dense_1.weights.copy()\n",
    "best_dense1_biases=dense_1.biases.copy()\n",
    "best_dense2_weights=dense_2.weights.copy()\n",
    "best_dense2_biases=dense_2.biases.copy()\n",
    "\n",
    "for iteration in range(10000):\n",
    "    # Generate new set of weights for iteration\n",
    "    dense_1.weights=0.05*np.random.randn(3, 2)\n",
    "    dense_1.biases=0.05*np.random.randn(1, 3)\n",
    "    dense_2.weights=0.05*np.random.randn(3, 3)\n",
    "    dense_2.biases=0.05*np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass for the training data\n",
    "    dense_1.forward(x)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    activation_2.forward(dense_2.output)\n",
    "\n",
    "    loss=loss_func.calculate(activation_2.output, y)\n",
    "\n",
    "    predictions=np.argmax(activation_2.output, axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if loss<lowest_loss:\n",
    "        print(f\"New set of weights found, Iteration {iteration}, Loss: {loss}, Accuracy: {accuracy}\")\n",
    "        best_dense1_weights=dense_1.weights.copy()\n",
    "        best_dense1_biases=dense_1.biases.copy()\n",
    "        best_dense2_weights=dense_2.weights.copy()\n",
    "        best_dense2_biases=dense_2.biases.copy()\n",
    "        lowest_loss=loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cccef",
   "metadata": {},
   "source": [
    "#### Randomly adjusting weights and biases - Works better but fails for complex data\n",
    "\n",
    "- If loss decreases for some `w` and `b`; choose the next value of weights and biases close to w, b.\n",
    "\n",
    "- If the loss increases, dont update weights, bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_1=DenseLayer(2, 3)\n",
    "activation_1=ActivationReLU()\n",
    "\n",
    "dense_2=DenseLayer(3, 3)\n",
    "activation_2=ActivationSoftmax()\n",
    "\n",
    "loss_func=CategoricalCrossEntropyLoss()\n",
    "\n",
    "# Helper variables\n",
    "lowest_loss=9999999\n",
    "best_dense1_weights=dense_1.weights.copy()\n",
    "best_dense1_biases=dense_1.biases.copy()\n",
    "best_dense2_weights=dense_2.weights.copy()\n",
    "best_dense2_biases=dense_2.biases.copy()\n",
    "\n",
    "for iteration in range(10000):\n",
    "    # Update weights with some small random values for iteration\n",
    "    dense_1.weights+=0.05*np.random.randn(3, 2)\n",
    "    dense_1.biases+=0.05*np.random.randn(1, 3)\n",
    "    dense_2.weights+=0.05*np.random.randn(3, 3)\n",
    "    dense_2.biases+=0.05*np.random.randn(1, 3)\n",
    "\n",
    "    # Perform a forward pass for the training data\n",
    "    dense_1.forward(x)\n",
    "    activation_1.forward(dense_1.output)\n",
    "    dense_2.forward(activation_1.output)\n",
    "    activation_2.forward(dense_2.output)\n",
    "\n",
    "    loss=loss_func.calculate(activation_2.output, y)\n",
    "\n",
    "    predictions=np.argmax(activation_2.output, axis=1)\n",
    "    accuracy=np.mean(predictions==y)\n",
    "\n",
    "    if loss<lowest_loss:\n",
    "        print(f\"New set of weights found, Iteration {iteration}, Loss: {loss}, Accuracy: {accuracy}\")\n",
    "        best_dense1_weights=dense_1.weights.copy()\n",
    "        best_dense1_biases=dense_1.biases.copy()\n",
    "        best_dense2_weights=dense_2.weights.copy()\n",
    "        best_dense2_biases=dense_2.biases.copy()\n",
    "        lowest_loss=loss\n",
    "    else:\n",
    "        dense_1.weights=best_dense1_weights.copy()\n",
    "        dense_1.biases=best_dense1_biases.copy()\n",
    "        dense_2.weights=best_dense2_weights.copy()\n",
    "        dense_2.biases=best_dense2_biases.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42eecd",
   "metadata": {},
   "source": [
    "### Back-Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68279746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "weights=np.array([-3.0, -1.0, 2.0])\n",
    "bias=1.0\n",
    "inputs=np.array([1.0, -2.0, 3.0])\n",
    "target_output=0.0\n",
    "learning_rate=0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0, 1.0, 0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "    # Forward pass\n",
    "    linear_output=np.dot(weights, inputs)+bias\n",
    "    output=relu(linear_output)\n",
    "    loss=(output-target_output)**2\n",
    "\n",
    "    # Backward pass\n",
    "    dloss_doutput=2*(output-target_output)\n",
    "    doutput_dlinear=relu_derivative(linear_output)\n",
    "    dlinear_dweights=inputs\n",
    "    dlinear_dbias=1.0\n",
    "\n",
    "    dloss_dlinear=dloss_doutput*doutput_dlinear\n",
    "    dloss_dweights=dloss_dlinear*dlinear_dweights\n",
    "    dloss_dbias=dloss_dlinear*dlinear_dbias\n",
    "\n",
    "    # Update weights and bias\n",
    "    weights-=learning_rate*dloss_dweights\n",
    "    bias-=learning_rate*dloss_dbias\n",
    "\n",
    "    print(f'Iteration {iteration+1}, Loss: {loss}')\n",
    "\n",
    "print(f'\\nFinal weights: {weights}')\n",
    "print(f'Final bias: {bias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2574059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With multiple neurons\n",
    "\n",
    "inputs=np.array([1, 2, 3, 4])\n",
    "weights=np.array([\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2]\n",
    "])\n",
    "biases=np.array([0.1, 0.2, 0.3])\n",
    "learning_rate=0.001\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0, 1.0, 0.0)\n",
    "\n",
    "for iteration in range(200):\n",
    "    z=np.dot(weights, inputs)+biases\n",
    "    a=relu(z)\n",
    "    y=np.sum(a)\n",
    "    loss=y**2\n",
    "\n",
    "    # Backward pass\n",
    "\n",
    "    # Gradient of loss w.r.t y\n",
    "    dL_dy=2*y\n",
    "    # Gradient of y w.r.t a\n",
    "    dy_da=np.ones_like(a)\n",
    "    # Gradient of loss w.r.t a\n",
    "    dL_da=dL_dy*dy_da\n",
    "    # Gradient of a w.r.t z\n",
    "    da_dz=relu_derivative(z)\n",
    "    # Gradient of loss w.r.t z\n",
    "    dL_dz=dL_da*da_dz\n",
    "\n",
    "    # Gradient of z w.r.t weights and biases\n",
    "    dL_dw=np.outer(dL_dz, inputs)\n",
    "    dL_db=dL_dz\n",
    "\n",
    "    weights-=learning_rate*dL_dw\n",
    "    biases-=learning_rate*dL_db\n",
    "\n",
    "    if iteration%20==0:\n",
    "        print(f'Iteration {iteration}, Loss: {loss}')\n",
    "\n",
    "print(f'\\nFinal weights: {weights}')\n",
    "print(f'Final biases: {biases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3808b",
   "metadata": {},
   "source": [
    "### Back-Propogation through Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19676a",
   "metadata": {},
   "source": [
    "#### Gradients of Loss w.r.t Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b724e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dz=np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0]\n",
    "])\n",
    "\n",
    "inputs=np.array([\n",
    "    [1, 2, 3, 2.5],\n",
    "    [2, 5, -1, 2],\n",
    "    [-1.5, 2.7, 3.3, -0.8]\n",
    "])\n",
    "\n",
    "dl_dweights=np.dot(inputs.T, dl_dz)\n",
    "print(dl_dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b0e89",
   "metadata": {},
   "source": [
    "#### Gradients of Loss w.r.t Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dz=np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0]\n",
    "])\n",
    "\n",
    "biases=np.array([[2, 3, 0.5]])\n",
    "\n",
    "dl_dbiases=np.sum(dl_dz, axis=0, keepdims=True)\n",
    "print(dl_dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e16c07d",
   "metadata": {},
   "source": [
    "#### Gradients of Loss w.r.t Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10852ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_dz=np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0]\n",
    "])\n",
    "\n",
    "weights=np.array([\n",
    "    [0.2, 0.8, -0.5, 1],\n",
    "    [0.5, -0.91, 0.26, -0.5],\n",
    "    [-0.26, -0.27, 0.17, 0.87]\n",
    "]).T\n",
    "\n",
    "dl_dinputs=np.dot(dl_dz, weights.T)\n",
    "print(dl_dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da332aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights=0.01*np.random.randn(n_neurons, n_inputs)\n",
    "        self.biases=np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs=inputs\n",
    "        self.output=np.dot(inputs, self.weights.T)+self.biases\n",
    "\n",
    "    def backward(self, dl_dz):\n",
    "        self.dweights=np.dot(self.inputs.T, dl_dz)\n",
    "        self.dbiases=np.sum(dl_dz, axis=0, keepdims=True)\n",
    "        self.dinputs=np.dot(dl_dz, self.weights.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
